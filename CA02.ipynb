{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FflVhR30vhe6"
      },
      "source": [
        "## CA02 Assignment ##\r\n",
        "Name: Karina Ramirez"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_-XLCuYrJnx",
        "outputId": "78a54c4d-a23f-4426-bff4-761fa064d9eb"
      },
      "source": [
        "# import packages \r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "# mount google drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852ALyvtgiIm"
      },
      "source": [
        "After importing the necessary packages, we create a function that builds a dictionary. This dictionary contains the 3,000 most common words. The function removes all non-alpha-numeric characters and any single alpha-numerica characters. Then, the function will repeatedly shrink the dictionary until it has the 3,000 most common words. Then, it will return the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ctGOSpnrkJD"
      },
      "source": [
        "# creating a dictionary that loops through emails and returns the 3,000 most common words\r\n",
        "def make_dictionary(root_directory):\r\n",
        "  # calling make_dictionary\r\n",
        "  all_words = []\r\n",
        "  # parsing through directory that contains emails\r\n",
        "  emails = [os.path.join(root_directory,f) for f in os.listdir(root_directory)]\r\n",
        "  for mail in emails:\r\n",
        "    with open(mail) as m:\r\n",
        "      for line in m:\r\n",
        "        words = line.split()\r\n",
        "        all_words += words\r\n",
        "        # splitting words and removing any single characters\r\n",
        "  dictionary = Counter(all_words)\r\n",
        "  list_to_remove = list(dictionary)\r\n",
        "\r\n",
        "  # removing all words that are nonalphabetical\r\n",
        "  for item in list_to_remove:\r\n",
        "    if item.isalpha() == False:\r\n",
        "      del dictionary[item]\r\n",
        "    elif len(item) == 1:\r\n",
        "      del dictionary[item]\r\n",
        "  # limit dictionary to most common 3,000 words\r\n",
        "  dictionary = dictionary.most_common(3000)\r\n",
        "  print(\"returning dictionary\")\r\n",
        "  return dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDFdHnEhDEL"
      },
      "source": [
        "After the dictionary has been created, we then make a function that extracts feature columns and then analyzes file names to determine if the emails are spam or not. The Labelled Data Column will be used in our ML to show the spam status. By extracting data from both the testing and training dataset, the function will return the feature dataset and label column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR6aIaPxrtI3"
      },
      "source": [
        "# this function parses through emails and extracts feature columns and analyzes file names to determine spam by creating a Labelled Data Column to be used in our ML\r\n",
        "def extract_features(mail_directory):\r\n",
        "  print(\"calling extract_features\")\r\n",
        "  files = [os.path.join(mail_directory,fi) for fi in os.listdir(mail_directory)]\r\n",
        "  # creating empty matrixes for our labels and features\r\n",
        "  features_matrix = np.zeros((len(files),3000))\r\n",
        "  train_labels = np.zeros(len(files))\r\n",
        "  count = 1;\r\n",
        "  docID = 0;\r\n",
        "  print(\"extracting features\")\r\n",
        "  for fil in files:\r\n",
        "    with open(fil) as fi:\r\n",
        "      for i, line in enumerate(fi):\r\n",
        "        if i ==2:\r\n",
        "          words = line.split()\r\n",
        "          for word in words:\r\n",
        "            wordID = 0\r\n",
        "            for i, d in enumerate(dictionary):\r\n",
        "              if d[0] == word:\r\n",
        "                wordID = i\r\n",
        "                features_matrix[docID,wordID] = words.count(word)\r\n",
        "      train_labels[docID] = 0;\r\n",
        "      filepathTokens = fil.split('/')\r\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\r\n",
        "      if lastToken.startswith(\"spmsg\"): # short-hand for spam message\r\n",
        "        train_labels[docID] = 1;\r\n",
        "        count = count + 1\r\n",
        "      docID = docID + 1\r\n",
        "  return features_matrix, train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loygil0Kry_3"
      },
      "source": [
        "# the directories where our data is housed \r\n",
        "train_directory = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\r\n",
        "test_directory = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_1OG7N0r26H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246e3cf6-8e1f-49a9-e02a-7de125e2af0b"
      },
      "source": [
        "# making dictionary using our emails \r\n",
        "dictionary = make_dictionary(train_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling make_Dictionary\n",
            "parsing through directory that contains emails\n",
            "removing all words that are nonalphabetical\n",
            "limit dictionary to most common 3,000 words\n",
            "returning dictionary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cliRJdbxsEE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109f3e3e-7dd4-4c34-9702-c89183f31045"
      },
      "source": [
        "print(\"extracting features and labels of our emails using the dictionary for our training and test sets\")\r\n",
        "features_matrix, labels = extract_features(train_directory)\r\n",
        "test_features_matrix, test_labels = extract_features(test_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "extracting features and labels of our emails using the dictionary for our training and test sets\n",
            "calling extract_features\n",
            "extracting features\n",
            "calling extract_features\n",
            "extracting features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z81cVfNFsGC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8977fee-8efc-4931-f926-4115610f3962"
      },
      "source": [
        "# initializing our Gaussian Naive Bayes\r\n",
        "model = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initializing our Gaussian Naive Bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QERtW6RYsIbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f028715-162e-4068-f581-a1d508bb514c"
      },
      "source": [
        "print (\"training our model\")\r\n",
        "model.fit(features_matrix, labels)\r\n",
        "print (\"training finished\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training our model\n",
            "training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9amwCarJsLXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e87c59-32b3-45cd-e5e9-42ba61ae4843"
      },
      "source": [
        "print (\"using our test set to test our model\")\r\n",
        "predicted_labels = model.predict(test_features_matrix)\r\n",
        "print (\"finished classification of the test data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using our test set to test our model\n",
            "finished classification of the test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJU6ESZbsPX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffc8942-d36a-4baf-f3ac-d1f7988a8e3e"
      },
      "source": [
        "print(\"printing accuracy of our test set for our training model\")\r\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing accuracy of our test set for our training model\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}